




## 대규모 채팅 애플리케이션 LIME 시스템

### [시스템 구성]
<img width="1180" height="1388" alt="image" src="https://github.com/user-attachments/assets/6f5732f4-afc9-4da9-8844-a3ed2fa2c4c8" />

### [요구사항]

1. 실시간성
- 롱폴링 대신 웹소켓 기반 양방향 지속 연결을 사용하여 서버와 클라이언트 간 실시간 메시지 송수신을 구현하여 불필요한 재연결 최소화 및 즉각 응답 처리  

2. 내결함성/내구성
- 3개 브로커를 서로 다른 개별 데이터센터(AZ : Availability Zone)에 분산 배치하고 카프카는 디스크에 순차 쓰기로 높은 성능과 내구성을 보장하며, AZ 장애 발생 시에도 다른 복제본에서 서비스 지속 제공 가능

3. 순서/일관성
- 카프카 파티션 키를 room_id(채팅방 고유 식별자)로 설정하여, 동일 채팅방의 모든 메시지가 동일 파티션으로 라우팅되도록 한다. 카프카는 파티션 내에서 메시지 순서를 자동 보장하므로 모든 사용자가 일관된 순서로 메시지 수신

4. 확장성
- 트래픽 증가 시 파티션과 컨슈머 수를 동적으로 확장하여 서비스 중단 없이 수평 확장 가능

5. 재접속 복구
- 최근 n시간의 메시지를 Redis에 캐싱하여, 클라이언트 재접속 시 마지막 수신 메시지의 시퀀스 번호를 통해 특정 시점 이후의 메시지 복구 가능


### [Kafka 클러스터 구성 내용]
- 파티션: 100개
- 브로커: 3개 (판교, 강남, 수원 데이터센터에 각 1개씩 배치)
   - 각 브로커는 약 33~34개 파티션의 리더 담당
   - 모든 파티션은 3개 브로커에 복제되어 특정 브로커 장애 시에도 서비스 연속성 보장
- 컨슈머 그룹: 10개 (각 컨슈머는 10개 파티션 담당)
   - 파티션에서 메시지를 읽어 Redis 캐싱 및 WebSocket 전송 수행
   - 트래픽 증가 시 컨슈머와 파티션 수를 확장하여 무중단 수평 확장 가능
